---
title: "Proyecto Grupal - Herramientas de Ciencias de Datos II"
author: 
  - Estudiantes
  - Luis Fernando Amey Apuy - C20470
  - Javier Hernández Navarro - C13674
  - Anthony Mauricio Jiménez Navarro - C24067
date: "2024-05-13"
output: 
  rmdformats::downcute:
    default_style: "dark"
    downcute_theme: "chaos"
---
```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

# Importación librerias
```{r, warning=FALSE, message=FALSE}
library(readr)
library(tidyverse)
library(lubridate)
library(reticulate)

```

# Función para hacer scrap de los datos de whatsapp
```{r}
scrap <- function(name){
  # setear la carpeta de la data
  setwd("data")
  
  # buscar el archivo
  ruta <- paste(name, ".txt", sep = '')
  texto <- read_file(ruta)

  # remover, si es posible, los \r, y trabajar en \n
  texto <- str_replace_all(texto, "\r", "")
  texto <- strsplit(texto, '\n')[[1]]
  
  # separar horas y mensaje
  temp <- str_split_fixed(texto, "\\] ", 2)  

  # realizar el dataframe con los datos iniciales
  datos <- data.frame(hora = numeric(length(texto)))
  datos$hora <- temp[,1]
  datos$autor <- numeric(length(texto))
  datos$mensaje <- temp[,2]
  
  # Dividir entre autor y mensaje
  temp <- str_split_fixed(datos$mensaje, ": ", 2)
  datos$autor <- temp[,1]
  datos$mensaje <- temp[,2]
  
  # Sacar los índices de los enters en línea
  indices <- as.numeric(rownames(datos[!grepl("\\[", datos$hora),]))
  
  # Operar estas líneas
  for (i in indices){
    datos$mensaje[i] <- datos$hora[i]
    datos[i,1:2] <- datos[i-1,1:2]
  }
  datos$mensaje <- as.character(datos$mensaje)
  
  # Eliminar el corchete extra
  datos$hora <- gsub("\\[", "", datos$hora)
  
  # Dividir en días y horas
  datos <- datos %>% separate(hora, into = c("dia", "hora"), sep = ", ")
  
  # Formato lubridate
  datos$dia <- as_date(dmy(datos$dia))
  
  # En caso de PM, o p. m. AM lo ignora
  datos<- datos %>% mutate(pm = grepl("[pP]", datos$hora))
  datos$hora <- str_replace_all(datos$hora, "p\\.\\s*m\\.", "")
  datos$hora <- str_replace_all(datos$hora, "a\\.\\s*m\\.", "")  
  
  # Moverse a hms de 24 horas
  datos$hora <- hms(datos$hora)
  datos$hora <- datos$hora + hours(datos$pm*12)
  
  # Quitar la columna extra
  datos <- datos %>% select(-pm)
  return(datos)
}
```

```{r, warning=FALSE}
gen <- scrap("gen")
```

# Funciones extra
```{r}
source("cod/r/borrador.R")

# imagen omitida
# sticker omitido
# Ubicación:
# https://
# Cambió tu código de seguridad con
# @
# Video omitido
# documento omitido 
# <Se editó este mensaje.>

```

# Funciones para analizar sentimientos en Python
```{r}
source_python("cod/python/sentimientos_borrador.py")
```

# Librerías de python
```{python}
from deep_translator import GoogleTranslator
import pandas as pd
import time
from textblob import TextBlob
import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from transformers import pipeline
```

# Traducciones
```{python}
traductor = GoogleTranslator(source = 'es', target = 'en')

t = time.time()
fijo = pd.DataFrame(traductor.translate_batch(r.lista))
print(time.time() - t)
```

# Guardar traducciones
```{r}
resultado["R"] = r.lista
guardar <- reticulate::py$fijo
write.csv2(guardar, "docs/traducido.csv")
```

# Lectura
```{python}

raw = r.gen
mensajes_traducidos = pd.read_csv('docs/traducido.csv',
                                  sep = ';',
                                  names = ['id', 'mensaje'])
mensajes_traducidos = mensajes_traducidos[1:].reset_index()
mensajes_traducidos['mensaje'] = mensajes_traducidos['mensaje'].transform(lambda x: x.fillna(raw["mensaje"]))

mensajes_traducidos['mensaje'] = mensajes_traducidos['mensaje'].apply(lambda x: str(x))
mensajes_traducidos = pd.DataFrame(mensajes_traducidos["mensaje"])
```

# Análisis de sentimientos 1
```{python}
sid = SentimentIntensityAnalyzer()
sentimientos = mensajes_traducidos['mensaje'].apply(lambda x:  sid.polarity_scores(x))
sentimientos = sentimientos.apply(lambda x: pd.Series(x))
mensajes_traducidos = pd.concat([mensajes_traducidos, sentimientos], axis = 1)
```

# Análisis de sentimientos 2
```{python}
sentimientos = mensajes_traducidos['mensaje'].apply(lambda x:  TextBlob(x).sentiment)
sentimientos = sentimientos.apply(lambda x: pd.Series(x))
mensajes_traducidos = pd.concat([mensajes_traducidos, sentimientos], axis = 1)
mensajes_traducidos.columns = ["mensaje", "negativo", "neutral", "positivo", "compuesto", "polaridad", "subjetividad"]
```

# Análisis de sentimientos 3
Este es el que más se tarda
```{python}
sentiment_analysis = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")
sentimientos = mensajes_traducidos['mensaje'].apply(lambda j: sentiment_analysis(j))
sentimientos = sentimientos.apply(lambda j: pd.Series(pd.Series(j)[0]))
mensajes_traducidos = pd.concat([mensajes_traducidos, sentimientos], axis = 1)
```






















