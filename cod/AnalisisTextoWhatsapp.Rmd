---
title: "Proyecto Grupal - Herramientas de Ciencias de Datos II"
author: 
  - Estudiantes
  - Luis Fernando Amey Apuy - C20470
  - Javier Hernández Navarro - C13674
  - Anthony Mauricio Jiménez Navarro - C24067
date: "2024-05-13"
output: 
  rmdformats::downcute:
    default_style: "dark"
    downcute_theme: "chaos"
---

```{r, warning=FALSE}
library(readr)
library(tidyverse)
library(lubridate)
library(reticulate)
```


```{r}
scrap <- function(name){
  # setear la carpeta de la data
  setwd("..")
  setwd("data")
  
  # buscar el archivo
  ruta <- paste(name, ".txt", sep = '')
  texto <- read_file(ruta)
  
  # remover, si es posible, los \r, y trabajar en \n
  texto <- str_replace_all(texto, "\r", "")
  texto <- strsplit(texto, '\n')[[1]]
  
  # separar horas y mensaje
  temp <- str_split_fixed(texto, "\\] ", 2)
  
  # realizar el dataframe con los datos iniciales
  datos <- data.frame(hora = numeric(length(texto)))
  datos$hora <- temp[,1]
  datos$autor <- numeric(length(texto))
  datos$mensaje <- temp[,2]
  
  # Dividir entre autor y mensaje
  temp <- str_split_fixed(datos$mensaje, ": ", 2)
  datos$autor <- temp[,1]
  datos$mensaje <- temp[,2]
  
  # Sacar los índices de los enters en línea
  indices <- as.numeric(rownames(datos[!grepl("\\[", datos$hora),]))
  
  # Operar estas líneas
  for (i in indices){
    datos$mensaje[i] <- datos$hora[i]
    datos[i,1:2] <- datos[i-1,1:2]
  }
  datos$mensaje <- as.character(datos$mensaje)
  
  # Eliminar el corchete extra
  datos$hora <- gsub("\\[", "", datos$hora)
  
  # Dividir en días y horas
  datos <- datos %>% separate(hora, into = c("dia", "hora"), sep = ", ")
  
  # Formato lubridate
  datos$dia <- as_date(dmy(datos$dia))
  
  # En caso de PM, o p. m. AM lo ignora
  datos<- datos %>% mutate(pm = grepl("[pP]", datos$hora))
  datos$hora <- str_replace_all(datos$hora, "p\\.\\s*m\\.", "")
  datos$hora <- str_replace_all(datos$hora, "a\\.\\s*m\\.", "")  
  
  # Moverse a hms de 24 horas
  datos$hora <- hms(datos$hora)
  datos$hora <- datos$hora + hours(datos$pm*12)
  
  # Quitar la columna extra
  datos <- datos %>% select(-pm)
  return(datos)
}
```

```{r, warning=FALSE}
gen <- scrap("gen")
```


```{r}
spammer <- function(df){
  num <- df%>% count(autor) %>% arrange(desc(n))
  return(num)
}
a<- spammer(gen)
```

```{r}
dia_concurrido <- function(df){
    num <- df%>% count(dia) %>% arrange(desc(n))
  return(num)
}
a <- dia_concurrido(gen)
```

```{r}
hora_concurrida <- function(df){
  df$hora <- hour(df$hora)
  num <- df %>% count(hora) %>% arrange(desc(n))
  return(num[1,1])
}
a <- hora_concurrida(gen)
```

```{r}
streak_dias <- function(df){
  num <- unique(df$dia)
  dif <- num[1:length(num)-1] - num[2:length(num)]
  df <- data.frame(dif = dif)
  tabla <- df %>%
    mutate(is_target = dif == -1,               
           group = cumsum(!is_target)) %>%         
    group_by(group) 
  agrupados <- tabla %>%
    summarise(length = sum(is_target)) %>%         
    filter(length > 0) %>%arrange(desc(length))
  maximo <- agrupados[1,] 
  primerdia <- 0
  for(i in 1:length(tabla$group)){
    if (tabla$group[i]!= maximo$group | tabla$is_target[i] == FALSE){
      next
    } else {
      if (primerdia == 0){
        primerdia <- i
      }
      ultimodia <-i
    }
  }

  maximo <- maximo$length
  primerdia <- num[primerdia]
  ultimodia <- num[ultimodia+1]
  return(list(maximo, primerdia, ultimodia))
}
a <- streak_dias(gen)
```

```{r}
el_graciosito <- function(df){
  df <- df %>% mutate(sticker = grepl("sticker omitido", df$mensaje))
  df <- df[df$sticker,]
  num <- df %>% count(autor) %>% arrange(desc(n))
  return(num)
}
a <- el_graciosito(gen)
```

```{r}
el_grafico <- function(df){
  df <- df %>% mutate(imagen = grepl("imagen omitida", df$mensaje))
  df <- df[df$imagen,]
  num <- df %>% count(autor) %>% arrange(desc(n))
  return(num)
}
a <- el_grafico(gen)
```

```{r}
el_waze <- function(df){
    df <- df %>% mutate(imagen = grepl("Ubicación:", df$mensaje))
  df <- df[df$imagen,]
  num <- df %>% count(autor) %>% arrange(desc(n))
  return(num)
}
a <- el_waze(gen)
```

```{r}
el_inseguro <- function(df){
    df <- df %>% mutate(imagen = grepl("Cambió tu código de seguridad con", df$mensaje))
  df <- df[df$imagen,]
  num <- df %>% count(autor) %>% arrange(desc(n))
  return(num)
}
a <- el_inseguro(gen)
```

```{r}
el_camarografo <- function(df){
    df <- df %>% mutate(imagen = grepl("Video omitido", df$mensaje))
  df <- df[df$imagen,]
  num <- df %>% count(autor) %>% arrange(desc(n))
  return(num)
}
a <- el_camarografo(gen)
```

```{r}
el_wikipedia <- function(df){
    df <- df %>% mutate(imagen = grepl("documento omitido", df$mensaje))
  df <- df[df$imagen,]
  num <- df %>% count(autor) %>% arrange(desc(n))
  return(num)
}
a <- el_wikipedia(gen)
```

```{r}
el_corrector <- function(df){
    df <- df %>% mutate(imagen = grepl("<Se editó este mensaje.>", df$mensaje))
  df <- df[df$imagen,]
  num <- df %>% count(autor) %>% arrange(desc(n))
  return(num)
}
a <- el_corrector(gen)
```

```{r}
el_link <- function(df){ # verificar doble
    df <- df %>% mutate(imagen = grepl("https://", df$mensaje))
  df <- df[df$imagen,]
  num <- df %>% count(autor) %>% arrange(desc(n))
  return(num)
}
a <- el_link(gen)
```

```{r}
el_jefe <- function(df){ # verificar doble
    df <- df %>% mutate(imagen = grepl("@", df$mensaje))
  df <- df[df$imagen,]
  num <- df %>% count(autor) %>% arrange(desc(n))
  return(num)
}
a <- el_jefe(gen)
```

# Funciones para analizar sentimientos en Python

```{python}
'''
Función que analiza sentimientos usando la librería 'nltk'.
Dado que el método utilizado está entrenado en inglés, primero se traducen los
mensajes usando la librería 'translate' (está limitada a una cantidad de traducciones)
'''
import pandas as pd
import time
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from translate import Translator

#nltk.download('vader_lexicon') # correr solo la primera vez

def sentimientos_nltk(df):
  
  df_pandas = pd.DataFrame(df)
  
  traductor = Translator(from_lang = 'es', to_lang = 'en')
  
  df_pandas['mensaje_ingles'] = df_pandas['mensaje'].apply(lambda i: traductor.translate(i)) # traduce los mensajes
  #df_pandas['mensaje_ingles'] = df_pandas['mensaje'].apply(lambda i: (print(i), traductor.translate(i))[1])

  sid = SentimentIntensityAnalyzer()

  df_pandas['sentimiento_nltk'] = df_pandas['mensaje_ingles'].apply(lambda j: sid.polarity_scores(j)['compound'])

  #del df_pandas['mensaje_ingles']

  return(df_pandas)


t = time.time()
r.gen = sentimientos_nltk(r.gen)
print(time.time() - t)
```

```{python}
from textblob import TextBlob

def analizar_sentimiento(txt):
  '''
  Función que analiza sentimientos de un texto usando la librería 'TextBlob'.
  '''
  blob = TextBlob(txt)
  sentimiento = blob.sentiment

  return(sentimiento[0])


def sentimientos_textblob(df):
  '''
  Función que analiza sentimientos de una columna de mensajes en un dataframe.
  Este método está entrenado para analizar sentimientos de mensajes en español.
  '''
  
  df_pandas = pd.DataFrame(df)
  df_pandas['sentimiento_textblob'] = df_pandas['mensaje'].apply(lambda j: analizar_sentimiento(j))
  
  return(df_pandas)
  

t = time.time()
r.gen = sentimientos_textblob(r.gen)
print(time.time() - t)
```

```{python}
from transformers import pipeline

def sentimientos_transformers(df):
  '''
  Función que analiza sentimientos usando la librería 'transformers'.
  Este método está entrenado para analizar sentimientos de mensajes en español.
  '''
  
  df_pandas = pd.DataFrame(df)
  sentiment_analysis = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")
  df_pandas['sentimiento_transformers'] = df_pandas['mensaje'].apply(lambda j: sentiment_analysis(j)[0]['score'])
  
  return(df_pandas)

t = time.time()
r.gen = sentimientos_transformers(r.gen)
print(time.time() - t)
```

















